## 编程作业

实时轨迹场景中使用Redis存储了当前车辆的实时经纬度数据，请使用Mysql替换Redis来存储实时经纬度数据。

需要开发代码，使用Spark Structured Streaming把实时经纬度数据写入Mysql.

## 实现

1. 创建数据库表

   ```sql
   DROP TABLE IF EXISTS car_gps;
   CREATE TABLE IF NOT EXISTS car_gps(
   	deployNum VARCHAR(50) COMMENT '调度编号',
   	plateNum VARCHAR(10) COMMENT '车牌号',
   	timeStr VARCHAR(20) COMMENT '时间戳',
   	lng VARCHAR(20) COMMENT '经度',
   	lat VARCHAR(20) COMMENT '纬度',
   	dbtime TIMESTAMP DEFAULT CURRENT_TIMESTAMP COMMENT '数据入库时间',
   	PRIMARY KEY(deployNum, plateNum, timeStr)
   )
   ```

2. 引入MySQL依赖

   ~~~pom
   <dependency>
       <groupId>mysql</groupId>
       <artifactId>mysql-connector-java</artifactId>
       <version>5.1.46</version>
   </dependency>
   <dependency>
       <groupId>org.apache.spark</groupId>
       <artifactId>spark-sql-kafka-0-10_2.12</artifactId>
       <version>${spark.version}</version>
   </dependency>
   ~~~

3. 定义BusInfo Bean对象

   ~~~scala
   package com.lg.bean
   
   //接收各个字段
   case class BusInfo(
                       deployNum: String,
                       simNum: String,
                       transportNum: String,
                       plateNum: String,
                       lglat: String,
                       speed: String,
                       direction: String,
                       mileage: String,
                       timeStr: String,
                       oilRemain: String,
                       weights: String,
                       acc: String,
                       locate: String,
                       oilWay: String,
                       electric: String
                     )
   
   object BusInfo {
   
     def apply(msg: String): BusInfo = {
       //获取一条消息，按照逗号切分，准备各个字段数据然后获取businfo对象
       val arr: Array[String] = msg.split(",")
       if (arr.length == 15) {
         BusInfo(
           arr(0),
           arr(1),
           arr(2),
           arr(3),
           arr(4),
           arr(5),
           arr(6),
           arr(7),
           arr(8),
           arr(9),
           arr(10),
           arr(11),
           arr(12),
           arr(13),
           arr(14)
         )
       }else{
         null
       }
     }
   }
   ~~~

4. 自定义JdbcWriter类，实现ForeachWriter

   ~~~scala
   package com.lg.monitor
   
   import java.sql.{Connection, DriverManager, PreparedStatement}
   import java.util.Properties
   import com.lg.bean.BusInfo
   import org.apache.spark.sql.ForeachWriter
   
   /**
    * @Author zhouhao
    * @create 2021-03-04 17:12 
    * 自定义JdbcWriter类，实现ForeachWriter
    */
   class JdbcWriter extends ForeachWriter[BusInfo] {
     var conn: Connection = _
     var statement: PreparedStatement = _
       
     override def open(partitionId: Long, epochId: Long): Boolean = {
       if (conn == null) {
         conn = JdbcWriter.openConnection
       }
       true
     }
   
     override def process(value: BusInfo): Unit = {
       //把数据写入mysql表中
       val arr: Array[String] = value.lglat.split("_")
       val sql = "insert into car_gps(deployNum,plateNum,timeStr,lng,lat) values(?,?,?,?,?)"
       statement = conn.prepareStatement(sql)
       statement.setString(1, value.deployNum)
       statement.setString(2, value.plateNum)
       statement.setString(3, value.timeStr)
       statement.setString(4, arr(0))
       statement.setString(5, arr(1))
       statement.executeUpdate()
     }
   
     override def close(errorOrNull: Throwable): Unit = {
       if (null != conn) conn.close()
       if (null != statement) statement.close()
     }
   }
   
   object JdbcWriter {
     var conn: Connection = _
     val url = "jdbc:mysql://linux123:3306/lg_logstic?useUnicode=true&characterEncoding=utf8"
     val username = "root"
     val password = "12345678"
     def openConnection: Connection = {
       if (null == conn || conn.isClosed) {
         conn = DriverManager.getConnection(url, username, password)
       }
       conn
     }
   }
   ~~~

5. 读取Kafka数据，实时写入MySQL数据库

   ~~~scala
   package com.lg.monitor
   
   import com.lg.bean.BusInfo
   import org.apache.spark.sql.{DataFrame, Dataset, SparkSession}
   
   /**
     * @author zhouhao
     * @create 2021-03-04 18:06    
     * 使用结构化流读取kafka中的数据,解析数据并写入MySQL表
     */
   object RealTimeProcess {
     def main(args: Array[String]): Unit = {
       System.setProperty("HADOOP_USER_NAME", "root")
       //1 获取sparksession
       val spark: SparkSession = SparkSession.builder()
         .master("local[*]")
         .appName(RealTimeProcess.getClass.getName)
         .getOrCreate()
       spark.sparkContext.setLogLevel("WARN")
       import spark.implicits._
       //2 定义读取kafka数据源
       val kafkaDf: DataFrame = spark.readStream
         .format("kafka")
         .option("kafka.bootstrap.servers", "linux121:9092,linux122:9092,linux123:9092")
         .option("subscribe", "lg_bus_info")
         .load()
       //3 处理数据
       val kafkaValDf: DataFrame = kafkaDf.selectExpr("CAST(value AS STRING)")
       //转为ds
       val kafkaDs: Dataset[String] = kafkaValDf.as[String]
       //解析出经纬度数据，写入MySQL
       //封装为一个case class方便后续获取指定字段的数据
       val busInfoDs: Dataset[BusInfo] = kafkaDs.map(BusInfo(_)).filter(_ != null)
   
       //将数据写入MySQL表
       busInfoDs.writeStream
         .foreach(new JdbcWriter)
         .outputMode("append")
         .start()
         .awaitTermination()
     }
   }
   ~~~

   

## 遇到的问题

创建表时一定要注意所在库的编码格式，如果不是utf8，一定要在创建表时制定编码格式，要不然表的编码格式默认为数据库的默认编码格式，当我们插入的数据存在中文时，会报错

~~~text
Error: java.io.IOException: java.sql.SQLException: Incorrect string value: '\xE4\xBA\xACA11...' for column 'plateNum' at row 1
~~~

